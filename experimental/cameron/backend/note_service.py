import os
import random
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()


class LLMSummaryRequest(BaseModel):
    text: str
    prompt: str = None  # Optional custom prompt, uses default if not provided
    provider: str = "openai"  # Which LLM provider to use (openai, claude, gemini)
    api_key: str = None  # Optional API key, uses env var if not provided


# --- LLM IMPLEMENTATION CODE ---

import os
import requests
from openai import OpenAI
import anthropic
import google.generativeai as genai
import re
from dotenv import load_dotenv

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """
The following is a conversation about a shot.

Create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached,
such as a shot being approved (finalled) by the creative lead.

Just generate short/consise notes from the given conversation, without any header or footer text such as subject line or follow up questions.

Following is the conversation:
{conversation}

"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20",
}


def summarize_openai(conversation, model, client, custom_prompt=None):
    # Use custom prompt if provided, otherwise use default template
    if custom_prompt:
        prompt = f"{custom_prompt}\n\nFollowing is the conversation:\n{conversation}"
    else:
        prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT if not custom_prompt else custom_prompt},
            {"role": "user", "content": prompt if not custom_prompt else conversation},
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content


def summarize_claude(conversation, model, client, custom_prompt=None):
    if custom_prompt:
        prompt = f"{custom_prompt}\n\nFollowing is the conversation:\n{conversation}"
        system_prompt = custom_prompt
    else:
        prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
        system_prompt = SYSTEM_PROMPT

    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt if not custom_prompt else conversation},
        ],
    )
    return response.content[0].text


def summarize_ollama(conversation, model, client, custom_prompt=None):
    if custom_prompt:
        prompt = f"{custom_prompt}\n\nFollowing is the conversation:\n{conversation}"
    else:
        prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)

    response = client.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False},
    )
    return response.json()["response"]


def summarize_gemini(conversation, model, client, custom_prompt=None):
    if custom_prompt:
        full_prompt = f"{custom_prompt}\n\nFollowing is the conversation:\n{conversation}"
    else:
        full_prompt = f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"

    response = client.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        ),
    )
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    candidate = response.candidates[0]
    if candidate.finish_reason == 2:
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:
        raise Exception("Response blocked for other reasons")
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    return candidate.content.parts[0].text


def create_llm_client(provider, api_key=None, model=None):
    provider = provider.lower()
    if provider == "openai":
        if not api_key:
            raise ValueError("OpenAI requires an api_key.")
        return OpenAI(api_key=api_key)
    elif provider == "claude":
        if not api_key:
            raise ValueError("Anthropic Claude requires an api_key.")
        return anthropic.Anthropic(api_key=api_key)
    elif provider == "ollama":
        return requests.Session()
    elif provider == "gemini":
        if not api_key:
            raise ValueError("Gemini requires an api_key.")
        if not model:
            raise ValueError("Gemini requires a model name.")
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")


# --- LLM client cache ---
# Initialize all available LLM clients
openai_api_key = os.getenv("OPENAI_API_KEY")
openai_model = DEFAULT_MODELS["openai"]
openai_client = None
if openai_api_key:
    try:
        openai_client = create_llm_client(
            "openai", api_key=openai_api_key, model=openai_model
        )
        print("OpenAI client initialized successfully")
    except Exception as e:
        print(f"Error initializing OpenAI client: {e}")

gemini_api_key = os.getenv("GEMINI_API_KEY")
gemini_model = DEFAULT_MODELS["gemini"]
gemini_client = None
if gemini_api_key and gemini_api_key != "your-gemini-api-key":
    try:
        gemini_client = create_llm_client(
            "gemini", api_key=gemini_api_key, model=gemini_model
        )
        print("Gemini client initialized successfully")
    except Exception as e:
        print(f"Error initializing Gemini client: {e}")

anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
anthropic_model = DEFAULT_MODELS["claude"]
anthropic_client = None
if anthropic_api_key and anthropic_api_key != "your-anthropic-api-key":
    try:
        anthropic_client = create_llm_client(
            "claude", api_key=anthropic_api_key, model=anthropic_model
        )
        print("Anthropic client initialized successfully")
    except Exception as e:
        print(f"Error initializing Anthropic client: {e}")

DISABLE_LLM = os.getenv("DISABLE_LLM", "true").lower() in ("1", "true", "yes")


@router.post("/llm-summary")
async def llm_summary(data: LLMSummaryRequest):
    """
    Generate a summary using LLM for the given text.
    Tries providers in order: OpenAI -> Gemini -> Anthropic
    If no API keys are configured, returns error instead of fake data.
    """
    # Determine which client to use - either from request or env vars
    client_to_use = None
    model_to_use = None
    provider_name = None

    # If API key provided in request, create client dynamically
    if data.api_key:
        provider = data.provider.lower() if data.provider else "openai"
        try:
            if provider == "openai":
                client_to_use = create_llm_client("openai", api_key=data.api_key)
                model_to_use = DEFAULT_MODELS["openai"]
                provider_name = "openai"
            elif provider == "gemini":
                model_to_use = DEFAULT_MODELS["gemini"]
                client_to_use = create_llm_client("gemini", api_key=data.api_key, model=model_to_use)
                provider_name = "gemini"
            elif provider == "claude":
                client_to_use = create_llm_client("claude", api_key=data.api_key)
                model_to_use = DEFAULT_MODELS["claude"]
                provider_name = "claude"

            print(f"Using {provider} with provided API key")
        except Exception as e:
            print(f"Error creating client for {provider} with provided API key: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to create {provider} client: {str(e)}"
            )
    else:
        # Use pre-initialized clients from env vars
        has_any_client = openai_client or gemini_client or anthropic_client

        if not has_any_client:
            # No providers available - return error
            raise HTTPException(
                status_code=500,
                detail="No LLM providers available. Please configure at least one API key (OPENAI_API_KEY, GEMINI_API_KEY, or ANTHROPIC_API_KEY) in .env file or provide api_key in request.",
            )

        # Try OpenAI first (current preference)
        if openai_client:
            client_to_use = openai_client
            model_to_use = openai_model
            provider_name = "openai"
        elif gemini_client:
            client_to_use = gemini_client
            model_to_use = gemini_model
            provider_name = "gemini"
        elif anthropic_client:
            client_to_use = anthropic_client
            model_to_use = anthropic_model
            provider_name = "claude"

    # Generate summary with the selected client
    if client_to_use and provider_name:
        try:
            print(f"Using {provider_name} ({model_to_use}) for summary")
            if provider_name == "openai":
                summary = summarize_openai(data.text, model_to_use, client_to_use, data.prompt)
            elif provider_name == "gemini":
                summary = summarize_gemini(data.text, model_to_use, client_to_use, data.prompt)
            elif provider_name == "claude":
                summary = summarize_claude(data.text, model_to_use, client_to_use, data.prompt)
            else:
                raise HTTPException(status_code=500, detail=f"Unsupported provider: {provider_name}")

            return {"summary": summary}
        except Exception as e:
            print(f"Error with {provider_name}: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"{provider_name} failed: {str(e)}"
            )

    # No client available
    raise HTTPException(
        status_code=500,
        detail="No LLM client available. Please provide an API key.",
    )
